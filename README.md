# ğŸ§  ConFeval: Calibrated Fine-Tuning & Faithfulness Evaluation for LLMs  
**Author:** Aditi Prabakaran  
**Affiliations:** Undergraduate Research Intern @ IIIT-Hyderabad Â· B.Tech CSE @ SRM University (2027)  
[GitHub Repository](https://github.com/Aditi2k5/confeval)

---

## ğŸ” Overview  
**ConFeval** (Confidence-Faithfulness Evaluation) is a research-oriented framework designed to enhance the **faithfulness**, **confidence calibration**, and **interpretability** of Large Language Models (LLMs).  

This project explores *parameter-efficient fine-tuning (PEFT)* and *Low-Rank Adaptation (LoRA)* under quantization to improve model self-awareness and reduce hallucinations â€” making models not just *accurate*, but also *trustworthy*.

It evaluates models such as `google/gemma-2-2b-it` and `microsoft/phi-2` before and after **targeted data augmentation (DA)** to assess how fine-tuning affects factual consistency and calibration.

---

## ğŸ¯ Motivation  
Modern LLMs often produce *confidently incorrect* or *plausible but false* responses â€” a challenge in domains that demand high factual reliability.  
ConFeval aims to measure and mitigate such issues by integrating **faithfulness evaluation** and **confidence calibration** into fine-tuning workflows.  

The research explores how **data-aware fine-tuning** improves truthfulness while maintaining semantic coherence.

---

## ğŸ§© Key Features  
- ğŸ”§ **PEFT + LoRA Quantized Fine-Tuning** â€” lightweight, memory-efficient model adaptation  
- ğŸ“Š **Confidence & Faithfulness Metrics** â€” evaluates model truthfulness, ECE, and BERTScore F1  
- ğŸ§® **Case-Level Error Analysis** â€” tracks under/over-confidence, hallucination, and correction patterns  
- ğŸŒ€ **Sequential Learning Pipeline** â€” multi-stage adaptation (base â†’ aligned â†’ faithful model)  
- âš¡ **Targeted Data Augmentation** â€” enriches training sets to reinforce factual correctness  

---

## ğŸ“Š Sample Results  

### ğŸ”¹ `google/gemma-2-2b-it`
| Metric | Base | Post-DA | Î” (%) |
|:--|:--:|:--:|:--:|
| **Faithfulness** | 0.3593 | **0.5381** | +49.75 â†‘ |
| **BERTScore F1** | 0.8410 | **0.8582** | +2.04 â†‘ |
| **ECE (â†“)** | 0.0252 | **0.0374** | +48.3 â†‘ |
| **Mean Confidence** | 0.8159 | **0.8713** | +6.8 â†‘ |
| **Over-confident Errors** | 14 | **7** | âˆ’50 â†“ |

---

### ğŸ”¹ `microsoft/phi-2`
| Metric | Base | Post-DA | Î” (%) |
|:--|:--:|:--:|:--:|
| **Faithfulness** | 0.3395 | **0.3608** | +6.26 â†‘ |
| **BERTScore F1** | 0.7974 | **0.8319** | +4.33 â†‘ |
| **ECE (â†“)** | 0.0688 | **0.0588** | âˆ’14.49 â†“ |
| **Mean Confidence** | 0.7768 | **0.8546** | +10.01 â†‘ |

ğŸ§­ **Summary:**  
Faithfulness improvements of up to **+49%**, BERTScore F1 increase of **+4%**, and a 50% reduction in over-confident errors show how *targeted fine-tuning improves both reliability and calibration*.

---

## âš™ï¸ Installation & Usage  

```bash
# Clone the repository
git clone https://github.com/Aditi2k5/confeval.git
cd confeval

# Install dependencies
pip install -r requirements.txt

# Launch the Jupyter notebook for interactive analysis
jupyter notebook llm_faithfulness_targeted_da.ipynb

